[{"content":" # 变量与数据类型 # 声明变量 1 2 local a = 10 -- 局部变量 b = 20 -- 全局变量 # 数据类型 1 2 3 4 5 6 7 8 nil: 表示一个空值或未定义（在脚本语言中，变量为null等同于undefined） boolean: 包含两个值：true 和 false number: 双精度浮点数（lua中唯一的数值类型） string: 字符串 table: 表，Lua 唯一的数据结构 function: 函数 userdata: 用户自定义类型 thread: 线程 # 控制结构 # 条件语句 1 2 3 4 5 6 7 8 9 10 11 12 13 local x = 10 -- Lua中，不等号为 ~= if x~=10 then return end if x \u0026gt; 5 then print(\u0026#34;x is greater than 5\u0026#34;) elseif x == 5 then print(\u0026#34;x is equal to 5\u0026#34;) else print(\u0026#34;x is less than 5\u0026#34;) end # 循环 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 -- while 循环 local i = 1 while i \u0026lt;= 5 do print(i) i = i + 1 end -- for 循环 for j = 1, 5 do print(j) end -- 泛型 for 循环 local t = {10, 20, 30} for index, value in ipairs(t) do print(index, value) end -- do while循环(repeat until) local i = 1 repeat i = i + 1 until i\u0026gt;5 print(i) 使用 break 关键字可以跳出循环，但是Lua中并没有 continue 关键字可以直接进入下一次循环。\n# 函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -- 定义函数 function add(a, b) return a + b end -- 调用函数 local sum = add(3, 4) print(sum) -- 输出: 7 -- 匿名函数 -- lua中函数可以用变量存储，然后通过变量调用函数。 function_subtract = function(a, b) return a - b end print(function_subtract(10, 5)) -- 输出: 5 # 函数作用域 注意，lua中的函数和变量，包括函数中定义的变量，默认都是全局作用域，除非使用local关键字声明！\n1 2 3 4 5 6 7 8 9 -- 定义局部函数 local function add(a, b) -- 函数中定义的变量，默认是全局变量！ c=5 return a + b + c end print(add(3,4)) -- 12 print(c) -- 5 Lua 的核心理念之一是在尽可能最小的范围（本地）内声明变量和函数。\n所以定义变量和函数，以及在函数中定义变量时，尽可能使用local关键字。\n如果函数中声明了一个变量，并且需要在函数外使用该变量，应该在函数定义前，现将其声明为局部变量：\n1 2 3 4 5 6 local c local function add(a, b) -- 函数中定义的变量，默认是全局变量！ c=5 return a + b + c end # 表（Tables） 表是lua中唯一的数据结构，但非常灵活，可以实现复杂多样的数据结构，如数组，字典等。\n表中的元素使用逗号间隔。\n表元素不能使用local关键字声明，表元素的作用域与表一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -- 定义表 local person = { nilValue = nil, -- nil 空值或未定义 booleanValue = true, -- boolean numberValue = 42, -- number 双精度浮点数 stringValue = \u0026#34;Hello, World!\u0026#34;, -- string 字符串 tableValue = {1, 2, 3}, -- table 表 123， functionValue = function(x, y) return x + y end, -- function 函数 userDataValue = userdata, -- userdata（用户自定义类型） threadValue = coroutine.create(function() print(\u0026#34;Coroutine\u0026#34;) end), -- 表中可以同时包含数组元素和键值对 \u0026#34;element\u0026#34;, 123.89, {1, \u0026#34;two\u0026#34;, false} } 注意，为了兼容字典和数组类型，表把数组元素的索引作为其键。\n因此表可以分为两个部分，即数组部分和哈希部分。\n表中的数组部分是顺序序列，可以通过索引访问。lua中，索引从1开始计数！！！表中的数组元素默认按照先后顺序排列。 可以通过 “[整数]=value” 的键值对形式，显式地指定数组元素的索引。\n如：[5]=\u0026ldquo;five\u0026rdquo;，指定表的数组部分第5个元素为字符串“five”。\n哈希部分是键值对元素，只能通过点运算符直接访问。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 -- 通过 #table ，可以获取表中的数组部分的长度。 print(#person) -- 访问表的键值元素 print(person.numberValue) -- 输出: 42 -- 插入新元素 -- table.insert()方法，接收3个参数，表、索引和值，用于给表的数组部分插入值。索引不能大于 数组部分的长度+1，如果省略索引，则在数组部分的末尾插入。 table.insert(\u0026#34;key\u0026#34;, 6, \u0026#34;six\u0026#34;) -- 通过点运算符赋值的方式，插入键值对。 person.gender = \u0026#34;male\u0026#34; -- 遍历表 -- key, value in pairs() 获取表的所有键值对 for key, value in pairs(person) do print(key, value) end -- 遍历表的数组部分 for i = 1, #person do print(i, person[i]) end # 表函数的定义 使用匿名函数\n1 2 3 4 5 6 7 8 9 10 11 12 t={} --表外添加键值对 t.func=function(a,b) return a+b end --表内定义 t={ func=function(a,b) return a+b end } 直接定义\n1 2 3 4 5 t={} function t:func(a,b) return a+b end # 元表与元方法 拥有元方法的表称为元表，原方法是一些拥有特定名称的函数。\n元表允许改变表的行为，每个表都可以有一个元表，元表的元方法用于定义表的一些特定操作，如算术运算、索引、调用等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 -- 定义一个表 local t = {1, 2, 3} -- 定义一个元表 local mt = { -- __add是特定的元方法，用于定义两个表相加时，执行的操作。 __add = function(t1, t2) for i = 1, #t2 do table.insert(t1, t2[i]) end return t1 end } -- setmetatable函数，给表绑定元表 setmetatable(t, mt) -- 使用元方法 local t2 = {4, 5, 6} local t3 = t + t2 for _, value in ipairs(t3) do print(value) -- 输出: 1 2 3 4 5 6 end 元方法及其作用如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 __index:定义访问表中不存在的键时的行为。定义类时，通常使其等于类名（表名），否则无法通过对象名称访问成员方法。 __newindex:定义给表中不存在的键赋值时的行为。 __call:定义表作为函数调用时的行为。 __tostring:定义表转换成字符串时的值。 __len:定义对表使用操作符“#”时的行为。 -- 算术运算 __add:表相加 __sub:表相减 __mul:表相乘 __div:表相除 -- 逻辑判断(没有判断大于和大于等于的元方法) __eq:== __lt:\u0026lt; __le:\u0026lt;= # 面向对象编程 Lua 通过表和元表来实现面向对象编程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 -- 定义一个类 local Animal = {} Animal.__index = Animal -- 构造函数一般命名为new function Animal:new(name) -- 一般先用setmetatable函数，给一个空表绑定元表，然后返回该空表， -- 并在其基础上设置表元素。 local instance = setmetatable({}, Animal) instance.name = name return instance end -- 类的成员方法 function Animal:speak() print(\u0026#34;My name is \u0026#34; .. self.name) end -- 创建实例 local dog = Animal:new(\u0026#34;Buddy\u0026#34;) dog:speak() -- 输出: My name is Buddy 值得注意的是，new方法返回的对象仅仅是包含表中变量的一个新表，而类的成员方法定义在元表中，所以无法直接通过该对象访问元表中定义的成员方法。\n但由于对象与元表绑定，并且元表中定义了元方法 __index 为元表的名称，所以当通过该对象访问未定义的函数时，就会触发元方法，把调用对象替换成元表，从而实现通过对象调用类的成员方法。\n# 字符串操作 1 2 3 4 local s = \u0026#34;Hello, World\u0026#34; print(#s) -- #输出字符串长度 print(string.upper(s)) -- 转换为大写 print(string.lower(s)) -- 转换为小写 # require函数 1 require(\u0026#34;moduleName\u0026#34;) --注意模块名称为文件名省略.lua后缀。 require 函数用于加载并运行一个模块。如果模块之前已经加载过，require 会返回该模块的缓存结果，而不会重复加载。这意味着在同时引入该文件中的全局变量和函数。\n这个机制确保了模块在 Lua 脚本中只会被加载一次，避免了重复加载带来的性能问题和潜在的副作用。\n# 模块 模块用于将功能相关的代码封装在一个独立的命名空间中，从而避免命名冲突，模块也是使lua脚本之间进行数据传递和代码引用的重要机制，模块可以返回任意数据类型。\n定义模块时，使用点运算符创建该命名空间独有的变量，并且在lua文件中通过return语句返回模块。 每个lua脚本只能调用一次return语句（返回一个模块）\n模块通常是局部变量，因为如果是全局变量，就不必在当前文件return，也不必在其他文件require，其它文件可以直接使用全局变量（前提是这些文件建立了联系）。\n1 2 3 4 5 6 7 8 9 -- 定义模块(mymodule.lua) local M = {} M.value = 0 --通过点运算符，在表M的命名空间中定义了变量value function M.increment() M.value = M.value + 1 end -- 返回模块 return M 使用模块时，需要在当前lua脚本中，调用require函数并传入模块所在的文件名（不包含.lua后缀）。require函数返回模块。\n1 2 3 4 -- 使用模块(main.lua) local mymodule = require(\u0026#34;mymodule\u0026#34;) mymodule.increment() print(mymodule.value) -- 输出: 1 # 函数的覆盖与保留 有时候需要重写全局函数，同时能够调用原函数。可以先使用变量存储原函数，再重写原函数以覆盖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -- ISToolTipInv.render是来自其他文件的表函数 local original_render = ISToolTipInv.render function ISToolTipInv:render() -- 可以根据条件判断，调用原函数，还是重写后的函数。 if not CONDITION then original_render(self) end -- ... some custom code ... end function ISToolTipInv:render() -- 先执行原函数，再执行新的步骤 original_render(self) -- 新的代码： end # 作用域 全局与局部 全局变量和函数可以从任何文件、代码中的任何位置访问。\n局部变量只能从声明它们的文件或代码块访问。\n与函数不同，在表中定义的变量或函数等，作用域与表一致。\n虽然全局作用域是一个方便的功能，但它往往更成问题。访问全局变量速度较慢，而且存在被意外覆盖的风险。Lua 的核心理念之一是在尽可能最小的范围（本地）内声明变量和函数。\n# 特殊规则 声明变量时，只要没有使用local关键字，就一定是全局变量。\n声明函数时如果没有使用local关键字，一般是全局函数，除非该函数被封装在局部变量或局部表中：\n1 2 3 4 5 6 7 8 local func=function() print(\u0026#34;函数被封装在局部变量中，因此函数是局部函数。\u0026#34;) end local tb={} function tb:func() print(\u0026#34;函数被封装在局部表中，因此函数是局部函数。\u0026#34;) end 这符合访问权限的控制规则和一致性。\n# 跨文件访问全局变量 只需要在当前文件中引入目标文件，就可以直接访问目标文件中定义的全局变量（包含模块）。\n# 跨文件访问局部变量 局部变量的作用域是其所在的文件或代码块，因此无法直接跨文件访问，只能通过lua的模块机制或全局函数实现这一点。\n通过全局函数访问：\n1 2 3 4 5 6 -- file1.lua local localVar=1 function getLocalVar() return localVar end 1 2 3 4 -- file2.lua require (\u0026#34;file1\u0026#34;) -- 引入file1.lua中的全局函数 print(getLocalVar()) 通过模块访问：\n1 2 3 4 5 6 7 8 9 10 11 -- file1.lua local M={} local M.localVar=1 -- 模块内定义一个get函数，隐藏内部细节，实现数据封装 function M.getLocalVar() return M.localVar end return M 1 2 3 4 5 6 -- file2.lua local M=require(\u0026#34;file1\u0026#34;) print(M.getLocalVar()) return 虽然使用get和set方法有很多好处，但并非总是需要使用get和set方法访问模块或表内字段，尤其是需要频繁访问这些字段时，相比于直接访问字段，使用中间方法会降低性能。\n# 性能优化 代码块运行得越频繁，就越需要对其进行优化。\n# 作用域 尽可能使用local关键字声明变量和函数\n函数内部始终使用局部变量，如果函数外需要使用该局部变量，就现在函数外将其声明为局部变量。\n1 2 3 4 5 local x local function Fun() x = 5 return \u0026#34;something else\u0026#34; end 把文件中所有的全局作用域函数和变量，放入一个全局表中，避免全局命名空间的污染，便于使用和管理。\n把需要使用的全局表和函数存到局部变量中。Lua的最小作用域理念不仅适用于声明变量和函数，还适用于访问它们。通过将全局变量拉入本地空间，可以提高性能。\n1 2 3 4 5 6 7 local MyTable = {} function MyTable.Fun(table_list) for _, num in ipairs(table_list) -- 每次循环都要调用全局函数print和ZombRand print(ZombRand(num)) end end 把全局变量引入到本地后：\n1 2 3 4 5 6 7 8 local MyTable = {} function MyTable.Fun(table_list) local print = print local ZombRand = ZombRand for _, num in ipairs(table_list) print(ZombRand(num)) end end 更极端的做法是：\n1 2 3 4 5 6 7 8 9 local MyTable = {} local print = print local ZombRand = ZombRand local ipairs = ipairs -- 此时完全不需要在全局空间中查找 function MyTable.Fun(table_list) for _, num in ipairs(table_list) print(ZombRand(num)) end end 为了控制本地命名空间的污染，可以使用 do end 代码块，进一步改进：\n1 2 3 4 5 6 7 8 9 10 11 12 local MyTable = {} local ipairs = ipairs local print = print do -- 将非系统全局变量放入代码块中，避免本地命名空间污染 local ZombRand = ZombRand function MyTable.Fun(table_list) for _, num in ipairs(table_list) print(ZombRand(num)) end end end # 复合条件的顺序 考虑以下代码：\n1 2 3 if x and y then -- do something end 由于and和or逻辑判断是短路的，所以复合条件的顺序一定程度上能够控制逻辑判断次数。\n如果实现知道条件的真假概率，显而易见：\n对于and逻辑，应该把结果为false的可能性较大的条件放在前面。\n对于or逻辑，应该把结果为true的可能性较大的条件放在前面。\n# 避免重复调用 1 if getPlayer():getInventory():contains(\u0026#34;MyMod.MyItem\u0026#34;) and getPlayer():getInventory():contains(\u0026#34;MyMod.MyItem\u0026#34;) then 如果要多次使用同一个函数的返回值，确定其不变的情况下，将结果缓存在局部变量中：\n1 2 local inventory = getPlayer():getInventory() if inventory:contains(\u0026#34;MyMod.MyItem\u0026#34;) and inventory:contains(\u0026#34;MyMod.MyItem\u0026#34;) then # 参考资料 https://github.com/FWolfe/Zomboid-Modding-Guide/blob/master/api/README.md\n","date":"2024-05-24T00:00:00Z","image":"https://CHmadoka.github.io/p/lua%E5%9F%BA%E7%A1%80/logo_hu1821a5919a2fa887ac0823b98aca8095_9893_120x120_fill_box_smart1_3.png","permalink":"https://CHmadoka.github.io/p/lua%E5%9F%BA%E7%A1%80/","title":"Lua基础"},{"content":" # 烦躁，是在言说 最近感觉很烦躁，一方面对每天坐在电脑前消磨时光感到厌倦，但又因无事可做很容易不自觉地陷在这种状态里，有些麻木和压抑。另一方面，还有一些课要上，一些事要做，但是却对此感到很有压迫感，只想要逃离，让自己静下来，最好没有任何东西来打扰我的这种脆弱的安宁。\n这当然是不好的状态，也是一种言说：我对于当前并不满意，需要改变现状才行。\n反应出来的问题隐隐约约还可以说很多，我觉得应该可以简单的用一句话概括，即“我想要成为我自己”。\n无所事事消磨时光不是我内心所希望的，离经叛道地说，按部就班地上课，过分认真地学习也不是我希望的，为了让自我不会死去，我才自动用逃避和僵化应对环境。代价就是，我对于一切的感知都变得迟钝，无法捕捉到自己的想法和感受，也丧失了行动力。\n所以我究竟想要做什么？我应该更多地自我觉察，仔细地听心底的声音。\n# 应该、可能、理论上\u0026hellip;\u0026hellip; 前天与两个学弟一起进行课程答辩，我在谈话中又发现了自己的一个特点，即在回答问题时，习惯性使用限定词，并且大多数时候都是不必要的，比如“应该”、“可能”、“理论上”等。\n如果是一般人，在随意的交谈中大概不会如此严谨，即使是一些简单的问题，我也要深思熟虑，试图回答的完美无缺，除了需要认真思考得出结论，还有太多的算计。要不要说？能不能说？说了对方会怎么想？我又该怎么解释？解释以后对方对我有什么看法？为了避免问题复杂化怎么说比较妥当？\n仅仅从聊天上，就体现了我人格层面的一些问题：出于各种原因，例如过分在意他人看法，完美主义等，我给自己施加了太多的束缚，带来了极大的心理压力。\n# Last 毕业答辩的事情基本上全部结束了，似乎我的压力一下减轻了不少，但我却并没有因此感到轻松一点，反而仍然感觉背负着沉重的压力，所以我无力再应对任何事情，只想不被任何事打扰，维持这种状态就好。\n我一定是缺少人际关系，有一种想要与活生生的人更多的接触的愿望。\n另外，如果我找到一些方式，例如做自己真正想做的事，在一定的范围内越狂越好，把压抑和躁动全部发泄出来，把攻击性与生命力全部释放出来，活出自我与创造性，一定会变得很不一样。\n只是我目前还做不到，也许我把自己想的太弱小了？二十多岁的我可能还活在不知道童年的哪个时段吧。\n","date":"2024-05-15T00:00:00Z","permalink":"https://CHmadoka.github.io/p/%E8%BA%81%E5%8A%A8%E5%8E%8B%E6%8A%91%E9%80%83%E9%81%BF/","title":"躁动、压抑、逃避"},{"content":"最近又在网络上看到类似的内容，即大脑运作主要依赖两种模式网络： 默认模式网络（DMN）和执行控制网络（ECN）。\n简单地说，当我们大脑相对空闲，没有特定任务的时候，DMN占主导地位，大脑会倾向于自我察觉、反省、记忆、联想和想象，甚至是反刍思维。而当我们明确地思考、决策、集中注意力时，ECN会更活跃。\nDMN的存在，很好的解答了我的疑惑——为什么想要给自己放假，什么都不做完全的放松下来时，会感到心力交瘁，无法享受悠闲？为什么一个人无所事事的时候，总会忍不住胡思乱想？而且是对过去的回忆和对未来的幻想。因为大脑始终是活跃的，持续运转着，就像一台发动的内燃机，什么也不做的时候，大脑的DMN仍然活跃着，如果DMN长期活跃，让人一直沉浸在对过去的纠结，对未来的担忧，以及尚未付诸实践的幻想中，会显著地增加焦虑感、无价值感等负面情绪。\n相反，如果让自己有事可做、忙碌起来，ECN的活跃时间更多，大脑就会专注于问题本身，而无暇产生多余的忧虑，并且研究显示适当的忙碌能给人带来更多的充实和满足，以及幸福感等正向反馈。\n这与精神分析的观点不谋而合，精神分析认为人需要做一些事情，使自己的攻击性和力比多正常向外投注，同时可以获得满足感，否则这些能量就只能指向自身，造成自我压抑与攻击，产生不良情绪。弗洛伊德几乎是靠自己的想象提出了一套架空的理论，却能被现代神经科学的研究部分印证，这就是精神分析的魅力。\n另外，我还浏览到一些科学的休息方法。对于体力劳动的人而言，睡觉也许是最好的休息方式，因为可以使肌肉得到放松，体力也会慢慢恢复。\n而对于脑力劳动的人而言并非如此，相反，过多的睡眠和闲暇，会使DMN长期活跃，导致内耗不断增加，这就是什么都不做反而觉得很累的原因。\n当我们持续进行某项活动时，大脑的部分脑区会越来越兴奋，但达到一定程度后，过度的兴奋会逐渐转变成抑制，这与我们的认知是一致的，长时间进行同样的活动，状态会越来越差。\n大脑的科学的休息方式是，更换用脑的方式，例如计算和逻辑推理，与阅读和看视频，大脑主要活跃的脑区显然是不同的，使用另外的脑区，原来活跃的脑区就能得到休息，同时不会导致DMN的活跃。所以不同科目之间交叉学习是一种很合理的安排，既能充分的利用时间，又不容易使大脑感到疲倦。\n当然实际情况更为复杂，看到有一种说法，把用脑导致的劳累分为三种。\n因为身体低兴奋度而导致的累，例如长期坐着办公，大脑虽然活跃，但身体却不是，这种情况就可以通过运动缓解劳累。\n因为意志力过度使用导致的累，例如做不想的事情时，就会消耗意志力，此时可以进行一些被动式的休闲进行回复，例如观看影视作品等。\n因为长时间重复导致的累，例如大脑长时间思考同一类问题，此时就应该转换用脑方式，使原来活跃的脑区得到休息。\n这都值得我尝试，因为我常常感到压力大，于是开始逃避问题，越是逃避，DMN就会让我的状况越糟糕，而且即便我们休息的时候，待解决的问题仍然停留在我们的脑海中，并没有消失。所以用一位动画导演的话说，就是“不要逃避”。\n","date":"2024-04-16T00:00:00Z","permalink":"https://CHmadoka.github.io/p/%E5%A4%A7%E8%84%91%E4%B8%BB%E8%A6%81%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F%E7%BD%91%E7%BB%9C/","title":"大脑主要有两种模式网络？"},{"content":"最近感觉压力很大，一个星期以来几乎什么事情也没做。其实并没有特别多的事情要做，即使我虚度了一个星期也没有怎么样，但是需要我去完成的事情只要存在，就是一个持续的压力源，会消耗我的精神。\n我绝对是单线程类型，几乎没办法并行处理多项任务，而且精力相当有限，常常很费力地做完一些事情以后，就感觉接近极限了，这个时候一旦松懈下来，就很难再投入学习和工作的状态中。这明显体现了我非常不灵活、僵化的一面，也许这是对某种环境的适应，我却因此很难适应社会环境。\n人确实很需要有点事做，即便是玩游戏呢，也是一个发泄自己生命力的窗口，否则无处流通的能量会反过来损害自己。对于我来说，明显的损害就是焦虑，睡眠质量差，多梦，明明睡了很久却因为多梦而有很大的消耗，感到头脑昏昏沉沉的。\n总体来看，我的行为方式，又开始以逃避为主导，逃避各种活动、任务，回避社交，拖延直到deadline。\n我大概明白自己的问题在哪，但没有更好的办法去改变，因为我不会再给自己多余的压力和指责，也许再休息两天，或者说下定决心继续开始，就能回到正轨上。至少过去是这样，只是有时候这个恢复期格外的漫长。\n我也越来越认同，真正的强大，一定是心理上的强大，一切的竞争，最终都是人格层面的竞争。\n最后分享一首歌曲，歌词意境非常优美，类似于“感时花溅泪，恨别鸟惊心”，带有极强的投射色彩，也在一定程度上符合我的心境。\n\u0026lt;\u0026lt;The End of the World\u0026gt;\u0026gt;\nWhy does the sun go on shining?\n为什么阳光依旧闪耀？\nWhy does the sea rush to shore?\n为什么海浪持续拍打海岸？\nDon\u0026rsquo;t they know\nit\u0026rsquo;s the end of the world?\n难道它们不知道世界末日来临了？\n\u0026lsquo;Cause you don\u0026rsquo;t love me anymore\n因为你已经不再爱我了\nWhy do the birds go on singing?\n为什么鸟儿仍然在歌唱？\nWhy do the stars glow above?\n为什么星星仍然在天空中闪烁？\nDon\u0026rsquo;t they know\nit\u0026rsquo;s the end of the world?\n难道它们不知道世界末日来临了？\nIt ended when I lost your love\n因为我已经失去了你的爱\nI wake up in the morning and I wonder\n我清晨醒来后思索着\nwhy everything\u0026rsquo;s the same as it was\n为什么在末日一切如常？\nI can\u0026rsquo;t understand\nno I can\u0026rsquo;t understand\n我不明白，真的不明白\nHow life goes on the way it does\n生命为什么还会继续运转下去\nWhy does my heart go on beating?\n为什么我的心还在跳动？\nWhy do these eyes of mine cry?\n为什么我的眼睛在哭泣？\nDon\u0026rsquo;t they know\nit\u0026rsquo;s the end of the world?\n难道它们不知道世界末日来临了？\nIt enden when you said goodbye\n就在你对我说再见的时候\n","date":"2024-04-15T00:00:00Z","permalink":"https://CHmadoka.github.io/p/fear/","title":"Fear"},{"content":"由于一些原因要学习神经网络，发现几乎所有资料的内容都比较含蓄，描述不清晰，结构不完整。几天学习下来很痛苦，决定结合自己的理解写个教程。（结果并非像我一开始想的那样对初学者友好，大概严格的公式理论仍然是绕不开的吧）\n# 生物神经元结构 # 神经元数学模型 该模型由三部分组成，即加权求和、线性动态系统和非线性函数映射。\n图中，$y_j$表示第$j$个神经元的输出，$\\theta_i$表示神经元$i$的阈值，$u_k$（k=1,2,\u0026hellip;,M）为外部输入，$a_{ij}$为神经元$j$到神经元$i$的权值。$b_{ik}$为外部输入$u_k$到神经元$i$的权值，计算时可以把外部输入看做某个神经元的输出（例如BP神经网络中的偏置节点）。\n任意神经元$i$的输入可以看做两部分：从$y_1$到$y_N$代表的其他神经元的输出，以及从$u_1$到$u_M$代表的外部输入。 该图为了形式上的统一，把神经元的阈值$\\theta_i$看作是以-1为外部输入时的权值，实际上很容易使初学者理解困难。\n# 加权求和的数学描述： 用$v_i(t)$，$y_j(t)$，$u_k(t)$分别表示$t$时刻时，神经元$i$的状态（因为神经元的输出决定其状态，所以有时神经元的输出又称为状态，这两个词可能混用），神经元$j$的输出，第$k$个外部输入，则有： $$ \\begin{equation}v_i(t)=\\sum_{j=1}^Na_{ij}y_j(t)+\\sum_{k=1}^Mb_{ik}u_k(t)-\\theta_i\\end{equation} $$\n其中，$\\begin{aligned}\\sum_{j=1}^Na_{ij}y_j(t)\\end{aligned}$表示$y_1$到$y_N$所有神经元的输出与权值乘积的和，代表其他神经元的输出对该神经元的直接影响。 $\\begin{aligned}\\sum_{k=1}^Mb_{ik}u_k(t)\\end{aligned}$表示$u_1$到$u_M$所有外部输出与权值乘积的和，代表外部输入（如人工干涉）对该神经元的直接影响。 $\\theta_i$为神经元$i$的阈值。 显然，所有输入大于阈值时，$v_i(t)\u0026gt;0$，神经元为兴奋状态，否则神经元为抑制状态。\n# 神经元的输入输出关系 用$u_i$表示神经元的输入，$y_i$表示神经元的输出，则神经元$i$的输入输出关系表示为：\n$$ \\begin{equation} y_i=f(u_i) \\end{equation} $$ 其中，$f$为激活函数。该式表明，所有神经元接收输入以后，把它作为参数传递给激活函数，得到该神经元的输出。\n# 激活函数 根据函数的图像是否为直线，激活函数可分为线性激活函数1和非线性激活函数。 激活函数的主要作用：\n把输入映射到特定区间或特征值，既能决定神经元输出的值域，又可以把输入映射到特征空间（特征值）。 如果激活函数是非线性函数，可以极大地增强神经元的表达能力，建立从输入到输出的非线性映射。（一个非线性函数可以是任意个数的非线性函数与线性函数的叠加，因此具有极强的抽象表达能力） 在上图中，$v_i$只是加权求和的结果，还没有经过激活函数处理，不是最终的输出。\n# 常用的 非线性激活函数 对于任意神经元$i$，设$x_i$为输入，则输出为$f(x_i)$。\n阶跃函数 $$ \\begin{equation}f(x_i)= \\begin{cases} 0\u0026amp;{x_i\u0026lt;0}\\\\ 1\u0026amp;{x_i\\ge0}\\ \\end{cases} \\end{equation} $$ $$ f(x_i)=\\begin{cases} -1\u0026amp;{x_i\u0026lt;0}\\\\ 1\u0026amp;{x_i\\ge0}\\ \\end{cases} $$ 不管神经元使用什么激活函数，对于神经网络最终的输出，都能够再使用一次阶跃函数实现从原始输出到特征值的映射。\nReLU函数 $$ f(x_i)=\\begin{cases} 0\u0026amp;{x_i\u0026lt;0}\\\\ x_i\u0026amp;{x_i\\ge0}\\ \\end{cases} $$ ReLU函数形式简单，并且没有饱和问题，运算速度快，收敛效果好，常用于卷积神经网络。\nS型函数 s型函数具有平滑、渐进和单调性，是最常用的非线性函数。\n(1) Sigmoid函数 $$ \\begin{equation}f(x_i)=\\frac{1}{1+e^{-\\alpha x_i}}\\end{equation} $$ 该函数值域为(0,1)，即Sigmoid函数把神经元的输出映射到(0,1)区间。 缺点是，当输入的绝对值大于某个值以后，过快进入饱和状态（函数值趋于0或1，不再有显著的变化），出现梯度消失（梯度趋近于0）的情况，会导致模型训练时收敛缓慢，效果不佳。\n(2) 双曲线正切函数 $$ f(x_i)=\\frac{1-e^{-\\alpha x_i}}{1+e^{-\\alpha x_i}} $$ 该函数值域为(-1,1)，即双曲线正切函数把神经元的输出映射到(-1,1)区间。 以上式子中的$\\alpha$用于控制函数斜率，实际使用该函数时，通常省略该参数。\n# 神经网络的分类 # 按结构分类 前馈型 前馈型神经网络中，可分为不同的层，每一层神经元只与相邻的层相连。神经元的信号传递是单向的， 各神经元接收前一个（层）神经元的输入，并输出给下一个（层）神经元，没有任何反馈。 例如BP神经网络。\n反馈型 在反馈型神经网络中，存在一些神经元的输出，经过若干个神经元的传递后，又返回作为这些神经元输入的情况。 例如Hopfield神经网络。\n# 按工作方式分类 如果神经网络中各神经元同时更新状态（输出），就是同步工作方式。 如果神经网络中各神经元按某种顺序，逐个更新状态（输出），每个神经元更新状态时，其他神经元状态不变，就是异步工作方式。\n# Hebb学习规则 如果某一突触两端的神经元同时处于兴奋状态，它们的连接权值应该增强。 两个神经元的输出$y_i$和$y_j$越大，神经元越兴奋，权值就应该增强的越多。 用$w_{ij}$表示神经元$i$与$j$之间的权值，$k$表示时刻，则： $$ w_{ij}(k+1)=w_{ij}(k)+\\alpha y_i(k)y_j(k)\\quad (\\alpha \u0026gt; 0) $$ 即$\\quad更新后的权值=当前权值+权值的改变量$。\n线性函数的通式为 $f=kx+b$\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-04-09T00:00:00Z","image":"https://CHmadoka.github.io/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1_hu69ae4b20123b44ac4941d164f555be9a_68333_120x120_fill_box_smart1_3.png","permalink":"https://CHmadoka.github.io/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","title":"ANN 神经网络基础"},{"content":"BP神经网络（Backpropagation Neural network），具有多层神经元，并且神经元的信号（输入输出）总是向前（由输入层指向输出层）传播，所以是多层向前网络。\n# BP神经网络的结构 设BP神经网络有m层。第一层是输入层，最后一层（第m层）是输出层，中间各层称为隐层。\n除了输出层以外，每一层都可以加上一个偏置节点（又称偏置单元，如图标注“+1”的圆圈）。偏置单元没有输入（没有其他神经元指向它），它的输出总是1。（相当于把外部输入“+1”看作是某个神经元的输出）。\n因为偏置节点与神经元存在差异，所以一般把神经元和偏置节点统称为节点。\n直觉上，偏置节点可以对下一层的神经元产生直接影响，而其他神经元因为经过了层层传递，对下一层的影响相对间接。\n偏置单元并不是必需的，但它可以为每个隐藏层和输出层中的神经元提供一个可学习的常数偏移量，这个偏移量允许神经网络更好地拟合数据集并提高模型的表达能力。\n并且偏置单元允许神经网络在输入为零时仍能有输出，在没有偏置单元的情况下，即使输入不为零，如果加权求和结果为零，那么神经元的输出也将为零（陷入得不到非零的有效输出，神经元无法激活的状况）。通过引入偏置项，即使输入为零，神经元也有可能激活，从而提高了网络的表达能力。\n# BP神经网络的输入输出关系 BP神经网络中，输入层作为数据的缓冲，不对数据做任何处理。也就是说，输入的数据直接作为输入层神经元的输出，则： $$ y_i^1=x_i\\quad(i=1,2,\\cdots,p_1)\\ $$\n隐层和输出层的神经元，需要把神经元的输入作为参数，传递给激活函数，得到的结果作为神经元的输出。实际需要解决的问题都有着复杂的非线性特征，所以激活函数都选择非线性函数。\n隐层$k$与输出层各个神经元的非线性输入输出关系记为$f_k(k=2,\\cdots,m)$，由$k-1$层的第$j$个神经元到$k$层的第$i$个神经元的连接权值记为$w_{ij}^k$，第$k$层中第$i$个神经元的输入总和记为$u_i^k$,输出为$y_i^k$，则有： $$ y^k_i=f_k(u_i^k)\\ u_i^k=\\sum_jw_{ij}^{k}y_j^{k-1}\\quad(k=2,\\cdots,m) $$ 当从输入层输入数据$X=[x_1,x_2,\\cdots,x_{p_1}]^T$（设第n层有$p_n$个神经元，则输入层有$p_1$个神经元），得到输出层的输出为$Y=[y_1,y_2,\\cdots,y_{p_m}]^T$（输出层有$p_m$个神经元）。\n计算机科学中，常用$[x_1,x_2,\\cdots,x_{p_1}]$的形式表示一组序列，称为行向量（即水平方向的列表或者一维数组，向量是只有一个维度的矩阵）。而$T$代表线性代数中的矩阵转置操作，所以$[x_1,x_2,\\cdots,x_{p_1}]^T$代表一个列向量，其中的元素从上往下排列，与BP神经网络结构图中，纵向画出的神经元一一对应。\n例如输入层第$n$个节点的输入是$x_n$，输出层第$n$个节点的输出是$y_n$。\n因此，可以把BP神经网络看成是一个从输入到输出的非线性映射，从$p_1$个输入映射成$p_m$个输出。\n# BP定理 给定任意$\\varepsilon$\u0026gt;0，对于任意的连续函数$f$，存在一个三层前向神经网络，可以在任意$\\varepsilon$平方误差精度内逼近$f$。\n也就是说，只需要三层BP神经网络，就可以逼近任意一个连续的非线性函数，但是可能需要大量的隐层神经元个数，或者给BP神经网络添加更多的隐层数量。\n如何最有效地确定BP神经网络的结构尚无定论，但通常默认，输入层的神经元个数，与样本的特征数一致，使每一个输入层神经元对应一种特征。输入层的神经元个数，与样本的类别数一致，使每一个输出层神经元对应一种分类。\n# BP神经网络的运作过程 初始化网络，确定各层的节点数量，然后设置初始权值。 将数据归一化，然后从输入层输入训练数据集（包含输入与期望输出），开始学习。 前向（正向）传播：从输入层开始，逐层计算神经元的输入与输出，直到得出输出层的输出为止。 反向传播（学习）：根据实际输出与期望输出，先计算输出层的总误差（损失函数）和输出层神经元的误差信号，然后逆向逐层计算所有神经元的误差信号和权值修正量，直到更新完所有神经元的误差信号和权值。 重复前向传播和反向传播，直到损失函数或者输出层神经元的误差信号足够小（与给出的容差进行对比），或者达到固定的学习次数（每一次学习意味着先后进行一次前向传播和反向传播），就停止学习。同时使用这两个判断条件，可以有效避免欠拟合与过拟合的情况。 学习完成后，输入测试数据集，得到输出结果，将其映射为样本的具体类型。 # 权值初始化 在BP神经网络中，权值初始化是一个非常重要的步骤，它可以影响网络的收敛速度和性能。 常用的初始化方法如下：\n随机初始化 $$ w_{ij}^k=random(-a,a) $$ 所有权值表现为(-a,a)上的随机分布。\nXavier初始化 略\n# 损失函数 损失函数，又称为代价函数，误差函数，在机器学习和统计建模中用于度量模型预测值与真实值之间的差异或损失程度，在BP神经网络中，表示输出层的实际输出与期望输出之间的总误差。 常用的损失函数$J$如下：\n均方误差 $$ J=\\frac{1}{p_m}\\sum_{j=1}^{p_m}(y_j^m-\\hat{y_i})^2 $$ 计算所有输出层神经元的输出与期望输出之间的差的平方和，最后求平均。 由于最后不求平均也能反映误差水平，所以为了求导计算结果的美观，这里选择将其乘以$\\frac{1}{2}$，得到 $$ J=\\frac{1}{2}\\sum_{j=1}^{p_m}(y_j^m-\\hat{y_i})^2 $$ 下面将使用该式作为误差函数。 平均绝对误差 $$ J=\\frac{1}{p_m}\\sum_{i=1}^n\\lvert y_i^{p_m}-\\hat{y_i}\\rvert $$ # 梯度下降法 BP算法的目标就是要通过反向学习，改变权值，使输出层的输出结果越来越接近期望输出。这个问题等价于，求误差函数的极小值，约束条件是上述的输入输出关系。\n利用非线性规划中的“最快下降法（梯度下降法）”，使权值沿着误差函数的负梯度方向改变，得到神经网络权值的修正量为： $$ \\begin{equation}\\Delta w_{ij}^{k-1}=-\\varepsilon\\frac{\\partial J}{\\partial w_{ij}^{k-1}}\\quad(\\varepsilon\u0026gt;0)\\end{equation} $$ 其中，$\\varepsilon$为学习步长（又称为学习效率），它决定了每次对权值修正量的大小。 学习效率太大可能导致误差函数增大，难以或者无法收敛到最优解。学习效率太小，则会导致收敛缓慢，需要更多的学习次数和更多的计算，增加学习成本，还可能导致陷入局部最优解无法跳出。 一般学习效率小于0.5。\n梯度下降法中有一个重要概念，即误差梯度。在BP神经网络中，误差梯度表示由误差函数对所有权值的偏导数构成的向量，即\n$$ \\Delta E= \\left[\\frac{\\partial J}{\\partial w_{ij}^2},\\frac{\\partial J}{\\partial w_{ij}^3},\\frac{\\partial J}{\\partial w_{ij}^n},\\frac{\\partial J}{\\partial w_{ij}^m}\\right]\\quad \\left(\\begin{aligned}\u0026amp;n=2,3,\\cdots,m \\\u0026amp; i=1,2,\\cdots,p_n \\ \u0026amp;j=1,2,\\cdots,p_{n-1}\\end{aligned}\\right) $$ 其中，$p_n$表示第n层神经元的个数。\n可以看到，$权值的修正量=-学习效率\\times误差梯度元素$。 所以求所有权值的修正量，实际上就是求误差梯度。\n对上式求偏导（由于本文章面向初学者，偏向应用，省略复杂计算），得 $$ \\frac{\\partial J}{\\partial w_{ij}^{k-1}}=\\frac{\\partial J}{\\partial u_{i}^{k}}y_j^{k-1} $$\n记 $$\\begin{equation}d_i^k=\\frac{\\partial J}{\\partial u_i^k}\\end{equation}$$ 则 $$ \\Delta w_{ij}^{k-1}=-\\varepsilon d_i^ky_j^{k-1}\\quad(k=2,\\cdots,m) $$\n对 (2) 式求偏导，得 $$ \\begin{equation}d_i^k=\\frac{\\partial J}{\\partial y_i^k}f_k^{\u0026rsquo;}(u_i^k)\\end{equation} $$ 分两种情况求 (3) 式：\n对输出层（第m层），$k$=$m$，$y_i^k=y_i^m$，计算得 $$ d_i^m=(y_i^m-\\hat{y_i})f_m^{\u0026rsquo;}(u_i^m) $$ 对于隐层$k$，计算得 $$ d_i^k=f_k^{\u0026rsquo;}(u_i^k)\\sum_ld_l^{k+1}w_{li}^{k+1} $$ 综上，BP学习算法可归纳为一般式 $$ \\Delta w_{ij}^{k-1}=-\\varepsilon d_i^ky_j^{k-1}\\\\ d_i^m=(y_i^m-\\hat{y_i})f_m^{\u0026rsquo;}(u_i^m)\\\\ d_i^k=f_k^{\u0026rsquo;}(u_i^k)\\sum_ld_l^{k+1}w_{li}^{k+1}\\quad(k=m-1,\\cdots,2) $$ 为了便于求导，取激活函数$f_k$为Sigmoid函数，即 $$ y_i^k=f_k(u_i^k)=\\frac{1}{1+e^{-u_i^k}} $$ 把$y_i^k$与$f_k$代入一般式，则BP算法归纳为 $$ \\Delta w_{ij}^{k-1}=-\\varepsilon d_i^ky_j^{k-1}\\\\ d_i^m=y_i^m(1-y_i^m)(y_i^m-\\hat{y_i})\\\\ d_i^k=y_i^k(1-y_i^k)\\sum_ld_l^{k+1}w_{li}^{k+1}\\quad(k=m-1,\\cdots,2) $$ 可以看出，要求第$k$层节点的误差信号$d_i^k$，就要先求$(k+1)$层的误差信号$d_i^{k+1}$。求所有节点的误差信号，求所有权值，求误差梯度，三者是同时进行的。\n误差信号的求取是一个从输出层到输入层的反向传播的递归过程，所以称为反向传播（Backpropagation，BP）学习算法。\n# 输入输出的编码 在实际问题中，样本特征和分类都是抽象的描述，而神经网络的输入只能接收数值类型，并且输出也是数值类型。所以需要对它们进行编码。\n当然不是所有的样本特征都需要编码。比如以数列作为样本进行分类（如等比和等差数列等），其本身就是数值类型，所以无需编码，这个例子还有一个特殊的地方在于，数列的每一个单独的数值，都不能作为数列的特征，因为这些数值只有作为一个整体，才能表达数列的特征。\n# 样本特征的编码 很多实际问题中的样本特征，都是难以量化的，为此，需要把这些特征映射为特征值，才能作为输入。 特征值通常取{0,1}或者{-1,1}。\n例如，水果的分类问题：\n1 0 外形 圆 椭圆 质地 光滑 粗糙 质量 $\\le$ 300g $\u0026gt;$ 300g 任意一个水果，若特征值描述为$X=[x_1,x_2,x_3]$，其中，$x_1$，$x_2$，$x_3$分别对应外形、质地、质量三种特征。 则一个圆形，粗糙，质量小于300g的水果，其特征值为[1,0,1]。\n# 分类的编码 在分类问题中，需要为每一个分类设定一种编码（可称为名义输出），也就是指定分类所对应的输出层神经元的输出。\n输入学习样本时，为了保证每一个样本数据对应一种分类，需要同时输入一个名义输出作为期望输出，和一个标签作为分类的名称。一般把所有样本数据的集合称为数据集，所有标签的集合称为标签集。数据集、期望输出列表和标签集，是一一对应的。\n与特征值一样，名义输出值通常是整数，取{0,1}或者{-1,1}。\n这样，神经网络在学习过程中，不断缩小实际输出与期望输出（名义输出）的差值，就是不断地强化神经网络对样本的分类准确度；当神经网络输出了某个设定的名义输出，就可以判定样本的分类。\n由于一般输出层神经元数与分类数保持一致，所以可以让每一个神经元输出对应一种分类，只有一个输出可以取1，其他的只能取0，但输出1的神经元可以变化。例如在四分类问题中，四种名义输出分别为[1,0,0,0]，[0,1,0,0]，[0,0,1,0]，[0,0,0,1]。这样可以保证不同分类的输出，有两位是不同的1。\n如果输出层神经元数与分类数不一致，比如四分类问题中，输出层只有3个神经元，名义输出可以设置为[0,0,0]，[0,1,1]，[1,0,1]，[1,1,0]。这种选值方式也是为了保证不同分类的输出，有两位不同，所以没有使用[0,0,1]等其他值。\n直觉上，制造更大的区分度，更有利于神经网络对分类的判别。但这样的分类编码也存在一些问题，比如输出值可能与任何名义输出都不相同，则无法确定分类，这种情况也一定程度上反映了神经网络的学习效果不佳。\n实际上存在其他的分类方法，可以避免无法确定分类的情况，即使用softmax函数处理输出，把输出层的输出转换成概率分布，每一个神经元的输出，对应一种分类的概率，总能够选出概率最大的分类。 softmax函数如下： $$ f_i^m=\\frac{e^{y_i}}{\\begin{aligned}\\sum_{n=1}^{p_m}e^{y_n}\\end{aligned}}\\quad(i=1,2,\\cdots,p_m) $$ 其中$p_m$为输出层神经元的个数，$f_i^m$表示输出层的第$i$个神经元输出转换成概率的结果。\n# 输出到分类的映射 由于BP神经网络使用非线性激活函数，最终输出通常是小数的序列，而名义输出为整数序列，所以需要把输出层的输出映射成整数，通常使用非线性函数中的阶跃函数完成这种转换 $$ f(x_i)=\\begin{cases} 0\u0026amp;{x_i\u0026lt;0}\\\\ 1\u0026amp;{x_i\\ge0} \\end{cases} $$ $$ f(x_i)=\\begin{cases} -1\u0026amp;{x_i\u0026lt;0}\\\\ 1\u0026amp;{x_i\\ge0} \\end{cases} $$\n针对不同的名义输出，选择恰当的阶跃函数，就解决了这个问题。\n# 实例应用与代码 以下是一个使用BP神经网络，对数列进行分类的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 import numpy # 神经元类 class Neuron: def __init__(self, weights): # 神经元有一个权重列表，包含其到下一层神经元的权重 self.weights = weights self.output = 0 # 到下一层节点的输出 self.deviationSign = None # 误差信号 def __str__(self): return f\u0026#34;[误差信号：{self.deviationSign}, 输出：{self.output}, 权重：{self.weights}]\u0026#34; # 神经网络类 class NerveNet: def __init__(self, struct, offset=1): \u0026#34;\u0026#34;\u0026#34; 创建神经网络 :param struct: 各层神经元数量的数组（不包括每层的偏置节点），每层的实际节点数为struct[i]+1，输出层没有偏置节点！ :param offset: 偏置值 \u0026#34;\u0026#34;\u0026#34; layers = [] nodes = [] self.targets = None # 分类标签 self.desireValues = None # 期望值（分类对应的输出） # 遍历层 for i in range(len(struct) - 2): # 遍历节点(每层加上一个偏置节点） for j in range(struct[i] + 1): # 除了输出层，都要设置权值 if i != len(struct) - 1: nodes.append(Neuron([numpy.random.uniform(-0.1, 0.1) for x in range(struct[i + 1] + 1)])) else: nodes.append(Neuron(None)) nodes[-1].output = offset # 最后一个节点是偏置节点，输出永远为偏置值 layers.append(nodes) # 倒数第二层没有对偏置节点的权值 nodes = [] for i in range(struct[-2] + 1): nodes.append(Neuron([numpy.random.uniform(-0.1, 0.1) for x in range(struct[-1])])) nodes[-1].output = offset # 最后一个节点是偏置节点，输出永远为偏置值 layers.append(nodes) nodes = [] # 输出层没有权值和偏置节点 for i in range(struct[-1]): nodes.append(Neuron(None)) layers.append(nodes) self.layers = layers def __str__(self): str = \u0026#34;[\u0026#34; for i in range(len(self.layers)): str += \u0026#34;[\u0026#34; for j in range(len(self.layers)): str += __builtins__.str(self.layers[i][j]) str += \u0026#34;], \u0026#34; str += \u0026#34;]\u0026#34; return str def study(self, inputs, desireValues, targets, step, cycle=None, tolerance=None): \u0026#34;\u0026#34;\u0026#34; 开始学习 :param inputs: 训练集 :param desireValues: 期望输出 :param targets: 分类标签，与训练集的元素对应 :param step: 学习步长（效率） :param cycle: 学习周期数 :param tolerance: 容差 :return: 学习的终止条件有两种： 1.完成指定的学习周期数。 2.输出层的误差小于给定的容差。 完成指定的学习周期数： 优点： 简单直观：学习周期数是一个易于理解和控制的参数，因为它表示网络在整个数据集上迭代的次数。 可调性：可以根据经验或实验结果调整学习周期数，以平衡训练时间和模型性能。 缺点： 可能导致过拟合：如果学习周期数设置得太高，可能会导致模型在训练数据上过度拟合，而在测试数据上泛化性能不佳。 输出层的误差小于给定的容差： 优点： 直接衡量网络性能：输出层误差是对模型性能的直接度量，反映了网络在当前权重下的预测准确度。 精确控制：通过调整容差可以更精确地控制训练终止条件。 缺点： 受数据分布影响：输出层误差可能受到数据分布的影响，如果训练数据存在噪声或不均衡，可能会导致误差不稳定。 在实践中，通常会将这两个条件结合使用，以确保模型训练的有效性和稳健性。比如，在训练过程中同时监控学习周期数和输出层误差，当其中一个条件满足时停止训练。这样可以避免过度拟合，并在网络性能达到一定水平后及时停止训练，节省时间和计算资源。 \u0026#34;\u0026#34;\u0026#34; self.desireValues = desireValues self.targets = targets if cycle is not None: for i in range(cycle): for j in range(len(inputs)): self.forward(inputs[j]) self.backward(desireValues[j], step) elif tolerance is not None: # 使用容差时，先学习一遍，再进行判断 for j in range(len(inputs)): self.forward(inputs[j]) self.backward(desireValues[j], step) while self.getFinalDeviation(desireValues) \u0026gt; tolerance: for j in range(len(inputs)): self.forward(inputs[j]) self.backward(desireValues[j], step) # 把原始输出映射到特征空间，然后根据特征值判断分类 def getEigenValue(self): eigenvalue = [] for node in self.layers[-1]: if node.output \u0026lt; 0.5: eigenvalue.append(0) else: eigenvalue.append(1) return eigenvalue def work(self, dataset, targets): \u0026#34;\u0026#34;\u0026#34; 学习完成后的工作函数 :param dataset: 待测数据集 :param targets: 待测数据的真实标签 :return: \u0026#34;\u0026#34;\u0026#34; eigenvalue = None count=0 for index in range(len(dataset)): self.forward(dataset[index]) eigenvalue = self.getEigenValue() print(f\u0026#34;\\n输入为：{dataset[index]}，原始输出为：\u0026#34; f\u0026#34;{[self.layers[-1][x].output for x in range(len(self.layers[-1]))]}，特征值为：{eigenvalue}，\u0026#34;) for i in range(len(self.desireValues)): if numpy.array_equal(self.desireValues[i], eigenvalue): print(f\u0026#34;判断分类为 {self.targets[i]}，正确分类为 {targets[index]}\u0026#34;, end=\u0026#34;\\n\u0026#34;) if self.targets[i]==targets[index]: count+=1 break print(f\u0026#34;共{len(dataset)}个输入，有{count}个分类正确，正确率{count/len(dataset)*100:.2f}%\u0026#34;) # def sigmoid(self, input, function): # \u0026#34;\u0026#34;\u0026#34; # 激活函数 # :param input: 神经元的输入 # :param function: # :return: # \u0026#34;\u0026#34;\u0026#34; # if function.lower()==\u0026#34;sigmoid\u0026#34;: # # elif function.lower()==\u0026#34;\u0026#34; # # return numpy.exp(-input) def calculateOutput(self, layerNum, count): \u0026#34;\u0026#34;\u0026#34; 计算节点的输出 :param layerNum: 节点层数 :param count: 节点顺序 :return: \u0026#34;\u0026#34;\u0026#34; input = 0 node = self.layers[layerNum][count] # 节点的输入 = 上一层所有节点与权值乘积之和 \u0026#34;\u0026#34;\u0026#34; 注意！不同于一些其他语言，在python中循环使用的迭代变量具有函数作用域，而不是块作用域。 这意味着该变量在函数外可见。 如果在循环之前已经存在同名变量，循环会改变该变量的值！ \u0026#34;\u0026#34;\u0026#34; for each in self.layers[layerNum - 1]: input += each.output * each.weights[count] # 把节点输入作为参数，传给激活函数，得到节点的输出 node.output = 1 / (1 + numpy.power(numpy.e, -input)) \u0026#34;\u0026#34;\u0026#34; 输出语句 python中的浮点类型与整型计算时，不能使用+=符号 \u0026#34;\u0026#34;\u0026#34; # print(f\u0026#34;节点{layerNum+1}-{count+1}的输出: \u0026#34;+str(node.output)) # 向前传播 def forward(self, input): # 设置输入层的输出 for i in range(len(self.layers[0]) - 1): self.layers[0][i].output = input[i] # 遍历层 for i in range(1, len(self.layers)): # 遍历列（不包括偏置节点） for j in range(len(self.layers[i]) - 1): # 计算每个节点的输出 self.calculateOutput(i, j) # 因为输出层的最后一个节点不是偏置节点，所以单独计算 self.calculateOutput(len(self.layers) - 1, len(self.layers[-1]) - 1) def getFinalDeviation(self, desireValues): \u0026#34;\u0026#34;\u0026#34; 计算输出层的总误差 :param desireValues: 期望输出列表 :return: \u0026#34;\u0026#34;\u0026#34; value = 0 for i in range(len(self.layers[-1])): value += numpy.power(self.layers[-1][i].output - desireValues[i], 2) return value / 2 def getDeviationSign(self, layerNum, count, desireValue): \u0026#34;\u0026#34;\u0026#34; 计算各神经元的误差信号 :param layerNum: 层数 :param count: 顺序 :param desireValue: 期望输出 :return: \u0026#34;\u0026#34;\u0026#34; node = self.layers[layerNum][count] if layerNum == len(self.layers) - 1: node.deviationSign = node.output * (1 - node.output) * (node.output - desireValue[count]) else: node.deviationSign = (node.output * (1 - node.output) * sum(self.layers[layerNum + 1][x].deviationSign * node.weights[x] for x in range(len(self.layers[layerNum + 1])))) \u0026#34;\u0026#34;\u0026#34; 输出语句 \u0026#34;\u0026#34;\u0026#34; # print(f\u0026#34;节点{layerNum+1}-{count+1}的误差信号：{node.deviationSign}\u0026#34;) def updateWeights(self, step): \u0026#34;\u0026#34;\u0026#34; 权值学习； 前一层节点到后一层节点的权值修正量= -(步长)*后一层节点的误差信号*前一层节点的输出 :param layerNum: :param count: :param step: 学习步长 :return: \u0026#34;\u0026#34;\u0026#34; for i in range(len(self.layers) - 1): for j in range(len(self.layers[i])): # 遍历下一层的节点 for k in range(len(self.layers[i + 1])): # 计算误差梯度 correction = -step * self.layers[i + 1][k].deviationSign * self.layers[i][j].output # 权值学习 self.layers[i][j].weights[k] += correction \u0026#34;\u0026#34;\u0026#34; 输出语句 \u0026#34;\u0026#34;\u0026#34; # print(f\u0026#34;节点{i+1}-{j+1}的误差梯度：{correction}\u0026#34;) # print(f\u0026#34;节点{i+1}-{j+1}的权值更新：{self.layers[i][j].weights[k]}\u0026#34;) # for i in range(len(self.layers)): # for j in range(len(self.layers[i])): # print(self.layers[i][j].weights) def backward(self, desireValue, step): \u0026#34;\u0026#34;\u0026#34; 反向传播 :param desireValue: 期望输出 :return: \u0026#34;\u0026#34;\u0026#34; # 逆向遍历 range生成从len()-1到-1的逆序序列（步长为-1） for i in range(len(self.layers) - 1, -1, -1): for j in range(len(self.layers[i])): self.getDeviationSign(i, j, desireValue) self.updateWeights(step) struct = [4, 8, 3] # 输入层特征数，隐层神经元数，输出层分类数 train = [ [1, 2, 3, 4], [100, 101, 102, 103], [1, 3, 5, 7], [50, 52, 54, 56], [1, 4, 9, 16], [9, 16, 25, 36], [2, 4, 8, 16], [20, 40, 80, 160], #[10, 12, 14, 16], ] desire = [ [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 1, 1], [1, 1, 1] # [0,1,0] ] targets = [ \u0026#34;公差为1的等差数列\u0026#34;, \u0026#34;公差为1的等差数列\u0026#34;, \u0026#34;公差为2的等差数列\u0026#34;, \u0026#34;公差为2的等差数列\u0026#34;, \u0026#34;x的平方数列\u0026#34;, \u0026#34;x的平方数列\u0026#34;, \u0026#34;公比为2的等比数列\u0026#34;, \u0026#34;公比为2的等比数列\u0026#34;, # \u0026#34;公差为2的等差数列\u0026#34; ] test = [ [2, 3, 4, 5], [4, 6, 8, 10], [9, 16, 25, 36], [120, 122, 124, 126], [3,6,12,24] ] testTargets=[ \u0026#34;公差为1的等差数列\u0026#34;, \u0026#34;公差为2的等差数列\u0026#34;, \u0026#34;x的平方数列\u0026#34;, \u0026#34;公差为2的等差数列\u0026#34;, \u0026#34;公比为2的等比数列\u0026#34; ] nerveNet = NerveNet(struct) nerveNet.study(train, desire, targets, 0.1, 2000) nerveNet.work(test, testTargets) 信息论中的海明距离，用于描述两个等长编码之间不同位的个数，这里可以说海明距离为2。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-04-09T00:00:00Z","image":"https://CHmadoka.github.io/p/bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/1_hud271e26b886e24e34b864ebb2b9cf980_50512_120x120_fill_box_smart1_3.png","permalink":"https://CHmadoka.github.io/p/bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/","title":"BP神经网络与反向传播算法"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\n# Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\n# H1 # H2 # H3 # H4 # H5 # H6 # Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\n# Blockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\n# Blockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n# Blockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n# Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 # Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien # Code Blocks # Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] # One line code block 1 \u0026lt;p\u0026gt;A paragraph\u0026lt;/p\u0026gt; # List Types # Ordered List First item Second item Third item # Unordered List List item Another item And another item # Nested list Fruit Apple Orange Banana Dairy Milk Cheese # Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-09-07T00:00:00Z","permalink":"https://CHmadoka.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo theme Stack supports the creation of interactive image galleries using Markdown. It\u0026rsquo;s powered by PhotoSwipe and its syntax was inspired by Typlog.\nTo use this feature, the image must be in the same directory as the Markdown file, as it uses Hugo\u0026rsquo;s page bundle feature to read the dimensions of the image. External images are not supported.\n# Syntax 1 ![Image 1](1.jpg) ![Image 2](2.jpg) # Result Photo by mymind and Luke Chesser on Unsplash\n","date":"2023-08-26T00:00:00Z","image":"https://CHmadoka.github.io/p/image-gallery/1_hudd2e8259c20ed0dcaa856c253552e90f_14625_120x120_fill_q75_box_smart1.jpg","permalink":"https://CHmadoka.github.io/p/image-gallery/","title":"Image gallery"},{"content":"For more details, check out the documentation.\n# Bilibili video # Tencent video # YouTube video # Generic video file Your browser doesn't support HTML5 video. Here is a link to the video instead. # Gist # GitLab # Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― A famous person, The book they wrote Photo by Codioful on Unsplash\n","date":"2023-08-25T00:00:00Z","image":"https://CHmadoka.github.io/p/shortcodes/cover_huec3c3e34981507583e214021ad1b9a4b_12942_120x120_fill_q75_box_smart1.jpg","permalink":"https://CHmadoka.github.io/p/shortcodes/","title":"Shortcodes"},{"content":"Stack has built-in support for math typesetting using KaTeX.\nIt\u0026rsquo;s not enabled by default side-wide, but you can enable it for individual posts by adding math: true to the front matter. Or you can enable it side-wide by adding math = true to the params.article section in config.toml.\n# Inline math This is an inline mathematical expression: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n1 $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$ # Block math $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n1 2 3 $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi),e^{2 \\pi i \\xi x},d\\xi $$\n1 2 3 $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ ","date":"2023-08-24T00:00:00Z","permalink":"https://CHmadoka.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Welcome to Hugo theme Stack. This is your first post. Edit or delete it, then start writing!\nFor more information about this theme, check the documentation: https://stack.jimmycai.com/\nWant a site like this? Check out hugo-theme-stack-stater\nPhoto by Pawel Czerwinski on Unsplash\n","date":"2022-03-06T00:00:00Z","image":"https://CHmadoka.github.io/p/hello-world/cover_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://CHmadoka.github.io/p/hello-world/","title":"Hello World！！！"},{"content":" # 使用游戏版本：41.78.16 # 游戏目录文件结构 ProjectZomboid/ ├── media/ │ ├── actiongroups/ │ │ └── (包含描述动画过渡的 .xml 脚本) │ ├── anims_X/ │ │ └── (包含动画文件) │ ├── AnimSets/ │ │ └── (包含描述动画参数的 .xml 脚本) │ ├── lua/ │ │ └── (包含游戏的所有 Lua 代码) │ ├── maps/ │ │ └── (包含地图文件) │ ├── models_X/ │ │ └── (包含物品、武器和衣服的 3D 模型) │ ├── radio/ │ │ └── (包含描述电台操作的脚本) │ ├── scripts/ │ │ └── (包含描述物品、武器、车辆参数的脚本) │ ├── sound/ │ │ └── (包含游戏的音频文件) │ ├── texturepacks/ │ │ └── (包含打包的游戏纹理和图像) │ ├── textures/ │ │ └── (包含游戏未打包的纹理) │ └── ui/ │ └── (包含游戏的图标和图像) └── zombie/ └── (包含游戏的编译 Java 代码)\n# 用户目录文件结构 在Windows系统中，用户目录的路径通常为：C:/Users/username/Zomboid\nZomboid/（游戏根目录）\n├── console.txt（控制台日志文件）\n├── coop-console.txt（本地主机服务器日志文件）\n├── server-console.txt（专用服务器日志文件）\n├── Lua/（Lua脚本文件夹）\n├── mods/（本地存储的mod文件夹）\n├── Workshop/（Steam创意工坊文件夹）\n└── Saves/（游戏存档文件夹）\n# 模组文件结构 mod根目录/\n├── mod.info （包含mod及相关内容的描述）\n├── image.png （mod的封面图片，供mod.info引用）\n└── media/\n├── models_X/\n│ └── (3D 模型文件：.x, .fbx)\n├── scripts/\n│ └── (脚本文件：物品、车辆、配方、衣服)\n├── textures/\n│ └── (纹理图像文件：物品、衣服、车辆)\n├── ui/\n│ └── (图标、界面图像文件)\n├── texturepacks/\n│ └── (纹理包文件)\n├── lua/\n│ ├── shared/\n│ │ └── (服务器端和客户端共用的 Lua 文件)\n│ ├── client/\n│ │ └── (仅在客户端执行的 Lua 文件)\n│ └── server/\n│ └── (仅在服务器端执行的 Lua 文件)\n├── sound/\n│ └── (音频文件)\n├── maps/\n│ └── (地图文件)\n├── animSets/\n│ └── (描述动画参数的 XML 脚本)\n└── anims_X/\n└── (动画文件：.x, .fbx)\\\n# 游戏代码结构 PZ同时使用 Java 和 Lua。主要引擎和 API 功能使用 Java，大部分逻辑使用 Lua 脚本，便于在不需要编译的情况下进行修改。\n为了将Lua嵌入到Java中，PZ使用了se.krka.kahlua 库，这是一个纯Java编写的，轻量级Lua虚拟机，可以在java中解释执行Lua脚本，也可以在Lua中调用Java的类和方法。相比于LuaJ库可以把Lua编译成Java字节码，kahlua是解释执行Lua，性能相对较差。\nLua源代码位于 ProjectZomboid/media/lua 中，是完全公开的，包含三个子文件夹，用于客户端的 client 、用于服务器端的 server 以及两端共用的 shared ，有些lua文件包含一些开发者的注释。\nJava源代码位于 ProjectZomboid/Zombie 中，因为是编译过的.class文件，所以通过反编译才能阅读源代码。\nJava源代码中的部分API已经通过Lua公开，具体方式是，在lua文件中先声明一个全局的空表，然后在这个表中定义与Java API同名的空函数，公开的Java API都被写入了Lua文件的全局命名空间（表）中。\n这意味着在kahlua库的帮助下，在Lua中调用同名的Lua全局函数，实际上是间接调用Java中的类和方法，不需要任何额外操作，在任何lua文件中都可以调用，不同的是lua中使用冒号:代替Java中的.运算符访问成员变量和方法。\n如果要了解这些函数做了什么，就不得不阅读反编译的Java代码。\n因为lua中经常调用Java API，所以很可能返回一些Java对象，如ArrayList等，在lua中也要通过Java的方法来访问，例如：\n1 arrayList:get(0) 实际上，由于lua嵌入Java执行的需要，Java的很多原生方法在Lua中也是全局可用的，但这是因为开发者把它们写进了Lua的全局命名空间以及kahlua库的作用，而不是lua本身的特性。\n虽然Java拥有很多方便好用的方法，需要注意的是，通过lua间接调用Java会有一定的性能损失。\n# Lua源代码解析 # lua/shared 1 2 3 4 5 6 7 VehicleZoneDistribution.lua：更改或添加车辆生成逻辑 SpawnRegions.lua：加载用户目录中的出生点文件“servertest_spawnregions.lua” luautils.lua：定义了一系列实用的lua工具函数，包括仿Java的字符串处理函数，以及一些游戏内的与玩家、物品、方格、物品栏、耐久度等相关的操作函数。 keyBinding.lua：定义了游戏中键盘按键与功能的映射。 ISBaseObject.lua：定义了基本对象类，可创建对象，type属性默认为“ISBaseObject”，也可以传入一个字符串用于更改类型。 defines.lua：定义了一个全局表，包含很多物品掉落和玩家状态参数的键值对。 BodyLocations.lua：针对人物的各种服饰等，创建身体位置。各种衣物、包括绷带、伤口、甚至僵尸伤害都会占用身体位置，有些东西可以在其位置上存在多个（绷带、伤口、僵尸伤害）。还定义了一些衣物的互斥规则（不能同时穿戴）。 ","date":"0001-01-01T00:00:00Z","permalink":"https://CHmadoka.github.io/p/","title":""}]