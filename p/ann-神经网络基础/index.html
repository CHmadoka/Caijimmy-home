<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="由于一些原因要学习神经网络，发现几乎所有资料的内容都比较含蓄，描述不清晰，结构不完整。几天学习下来很痛苦，决定结合自己的理解写个教程。（结果并非像我一开始想的那样对初学者友好，大概严格的公式理论仍然是绕不开的吧） # 生物神经元结构 # 神经元数学模型 该模型由三部分组成，即加权求和、线性动态系统和非线性函数映射。 图中，$y_j$表示第$j$个神经元的输出，$\\theta_i$表示神经元$i$的阈值，$u_k$（k=1,2,&mldr;,M）为外部输入，$a_{ij}$为神经元$j$到神经元$i$的权值。$b_{ik}$为外部输入$u_k$到神经元$i$的权值，计算时可以把外部输入看做某个神经元的输出（例如BP神经网络中的偏置节点）。 任意神经元$i$的输入可以看做两部分：从$y_1$到$y_N$代表的其他神经元的输出，以及从$u_1$到$u_M$代表的外部输入。 该图为了形式上的统一，把神经元的阈值$\\theta_i$看作是以-1为外部输入时的权值，实际上很容易使初学者理解困难。 # 加权求和的数学描述： 用$v_i(t)$，$y_j(t)$，$u_k(t)$分别表示$t$时刻时，神经元$i$的状态（因为神经元的输出决定其状态，所以有时神经元的输出又称为状态，这两个词可能混用），神经元$j$的输出，第$k$个外部输入，则有： $$ \\begin{equation}v_i(t)=\\sum_{j=1}^Na_{ij}y_j(t)+\\sum_{k=1}^Mb_{ik}u_k(t)-\\theta_i\\end{equation} $$ 其中，$\\begin{aligned}\\sum_{j=1}^Na_{ij}y_j(t)\\end{aligned}$表示$y_1$到$y_N$所有神经元的输出与权值乘积的和，代表其他神经元的输出对该神经元的直接影响。 $\\begin{aligned}\\sum_{k=1}^Mb_{ik}u_k(t)\\end{aligned}$表示$u_1$到$u_M$所有外部输出与权值乘积的和，代表外部输入（如人工干涉）对该神经元的直接影响。 $\\theta_i$为神经元$i$的阈值。 显然，所有输入大于阈值时，$v_i(t)>0$，神经元为兴奋状态，否则神经元为抑制状态。 # 神经元的输入输出关系 用$u_i$表示神经元的输入，$y_i$表示神经元的输出，则神经元$i$的输入输出关系表示为： $$ \\begin{equation} y_i=f(u_i) \\end{equation} $$ 其中，$f$为激活函数。该式表明，所有神经元接收输入以后，把它作为参数传递给激活函数，得到该神经元的输出。 # 激活函数 根据函数的图像是否为直线，激活函数可分为线性激活函数1和非线性激活函数。 激活函数的主要作用： 把输入映射到特定区间或特征值，既能决定神经元输出的值域，又可以把输入映射到特征空间（特征值）。 如果激活函数是非线性函数，可以极大地增强神经元的表达能力，建立从输入到输出的非线性映射。（一个非线性函数可以是任意个数的非线性函数与线性函数的叠加，因此具有极强的抽象表达能力） 在上图中，$v_i$只是加权求和的结果，还没有经过激活函数处理，不是最终的输出。 # 常用的 非线性激活函数 对于任意神经元$i$，设$x_i$为输入，则输出为$f(x_i)$。 阶跃函数 $$ \\begin{equation}f(x_i)= \\begin{cases} 0&{x_i<0}\\\\ 1&{x_i\\ge0}\\ \\end{cases} \\end{equation} $$ $$ f(x_i)=\\begin{cases} -1&{x_i<0}\\\\ 1&{x_i\\ge0}\\ \\end{cases} $$ 不管神经元使用什么激活函数，对于神经网络最终的输出，都能够再使用一次阶跃函数实现从原始输出到特征值的映射。 ReLU函数 $$ f(x_i)=\\begin{cases} 0&{x_i<0}\\\\ x_i&{x_i\\ge0}\\ \\end{cases} $$ ReLU函数形式简单，并且没有饱和问题，运算速度快，收敛效果好，常用于卷积神经网络。 S型函数 s型函数具有平滑、渐进和单调性，是最常用的非线性函数。 (1) Sigmoid函数 $$ \\begin{equation}f(x_i)=\\frac{1}{1+e^{-\\alpha x_i}}\\end{equation} $$ 该函数值域为(0,1)，即Sigmoid函数把神经元的输出映射到(0,1)区间。 缺点是，当输入的绝对值大于某个值以后，过快进入饱和状态（函数值趋于0或1，不再有显著的变化），出现梯度消失（梯度趋近于0）的情况，会导致模型训练时收敛缓慢，效果不佳。"><title>ANN 神经网络基础</title>
<link rel=canonical href=https://CHmadoka.github.io/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/><link rel=stylesheet href=/scss/style.min.8e60baf4cd3fc55968717a6e39762f4d28ed7ef6007566b6c7970ad0fe907198.css><meta property='og:title' content="ANN 神经网络基础"><meta property='og:description' content="由于一些原因要学习神经网络，发现几乎所有资料的内容都比较含蓄，描述不清晰，结构不完整。几天学习下来很痛苦，决定结合自己的理解写个教程。（结果并非像我一开始想的那样对初学者友好，大概严格的公式理论仍然是绕不开的吧） # 生物神经元结构 # 神经元数学模型 该模型由三部分组成，即加权求和、线性动态系统和非线性函数映射。 图中，$y_j$表示第$j$个神经元的输出，$\\theta_i$表示神经元$i$的阈值，$u_k$（k=1,2,&mldr;,M）为外部输入，$a_{ij}$为神经元$j$到神经元$i$的权值。$b_{ik}$为外部输入$u_k$到神经元$i$的权值，计算时可以把外部输入看做某个神经元的输出（例如BP神经网络中的偏置节点）。 任意神经元$i$的输入可以看做两部分：从$y_1$到$y_N$代表的其他神经元的输出，以及从$u_1$到$u_M$代表的外部输入。 该图为了形式上的统一，把神经元的阈值$\\theta_i$看作是以-1为外部输入时的权值，实际上很容易使初学者理解困难。 # 加权求和的数学描述： 用$v_i(t)$，$y_j(t)$，$u_k(t)$分别表示$t$时刻时，神经元$i$的状态（因为神经元的输出决定其状态，所以有时神经元的输出又称为状态，这两个词可能混用），神经元$j$的输出，第$k$个外部输入，则有： $$ \\begin{equation}v_i(t)=\\sum_{j=1}^Na_{ij}y_j(t)+\\sum_{k=1}^Mb_{ik}u_k(t)-\\theta_i\\end{equation} $$ 其中，$\\begin{aligned}\\sum_{j=1}^Na_{ij}y_j(t)\\end{aligned}$表示$y_1$到$y_N$所有神经元的输出与权值乘积的和，代表其他神经元的输出对该神经元的直接影响。 $\\begin{aligned}\\sum_{k=1}^Mb_{ik}u_k(t)\\end{aligned}$表示$u_1$到$u_M$所有外部输出与权值乘积的和，代表外部输入（如人工干涉）对该神经元的直接影响。 $\\theta_i$为神经元$i$的阈值。 显然，所有输入大于阈值时，$v_i(t)>0$，神经元为兴奋状态，否则神经元为抑制状态。 # 神经元的输入输出关系 用$u_i$表示神经元的输入，$y_i$表示神经元的输出，则神经元$i$的输入输出关系表示为： $$ \\begin{equation} y_i=f(u_i) \\end{equation} $$ 其中，$f$为激活函数。该式表明，所有神经元接收输入以后，把它作为参数传递给激活函数，得到该神经元的输出。 # 激活函数 根据函数的图像是否为直线，激活函数可分为线性激活函数1和非线性激活函数。 激活函数的主要作用： 把输入映射到特定区间或特征值，既能决定神经元输出的值域，又可以把输入映射到特征空间（特征值）。 如果激活函数是非线性函数，可以极大地增强神经元的表达能力，建立从输入到输出的非线性映射。（一个非线性函数可以是任意个数的非线性函数与线性函数的叠加，因此具有极强的抽象表达能力） 在上图中，$v_i$只是加权求和的结果，还没有经过激活函数处理，不是最终的输出。 # 常用的 非线性激活函数 对于任意神经元$i$，设$x_i$为输入，则输出为$f(x_i)$。 阶跃函数 $$ \\begin{equation}f(x_i)= \\begin{cases} 0&{x_i<0}\\\\ 1&{x_i\\ge0}\\ \\end{cases} \\end{equation} $$ $$ f(x_i)=\\begin{cases} -1&{x_i<0}\\\\ 1&{x_i\\ge0}\\ \\end{cases} $$ 不管神经元使用什么激活函数，对于神经网络最终的输出，都能够再使用一次阶跃函数实现从原始输出到特征值的映射。 ReLU函数 $$ f(x_i)=\\begin{cases} 0&{x_i<0}\\\\ x_i&{x_i\\ge0}\\ \\end{cases} $$ ReLU函数形式简单，并且没有饱和问题，运算速度快，收敛效果好，常用于卷积神经网络。 S型函数 s型函数具有平滑、渐进和单调性，是最常用的非线性函数。 (1) Sigmoid函数 $$ \\begin{equation}f(x_i)=\\frac{1}{1+e^{-\\alpha x_i}}\\end{equation} $$ 该函数值域为(0,1)，即Sigmoid函数把神经元的输出映射到(0,1)区间。 缺点是，当输入的绝对值大于某个值以后，过快进入饱和状态（函数值趋于0或1，不再有显著的变化），出现梯度消失（梯度趋近于0）的情况，会导致模型训练时收敛缓慢，效果不佳。"><meta property='og:url' content='https://CHmadoka.github.io/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/'><meta property='og:site_name' content='CHmadoka 小屋'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='神经网络'><meta property='article:published_time' content='2024-04-09T00:00:00+00:00'><meta property='article:modified_time' content='2024-04-09T00:00:00+00:00'><meta property='og:image' content='https://CHmadoka.github.io/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1.png'><meta name=twitter:title content="ANN 神经网络基础"><meta name=twitter:description content="由于一些原因要学习神经网络，发现几乎所有资料的内容都比较含蓄，描述不清晰，结构不完整。几天学习下来很痛苦，决定结合自己的理解写个教程。（结果并非像我一开始想的那样对初学者友好，大概严格的公式理论仍然是绕不开的吧） # 生物神经元结构 # 神经元数学模型 该模型由三部分组成，即加权求和、线性动态系统和非线性函数映射。 图中，$y_j$表示第$j$个神经元的输出，$\\theta_i$表示神经元$i$的阈值，$u_k$（k=1,2,&mldr;,M）为外部输入，$a_{ij}$为神经元$j$到神经元$i$的权值。$b_{ik}$为外部输入$u_k$到神经元$i$的权值，计算时可以把外部输入看做某个神经元的输出（例如BP神经网络中的偏置节点）。 任意神经元$i$的输入可以看做两部分：从$y_1$到$y_N$代表的其他神经元的输出，以及从$u_1$到$u_M$代表的外部输入。 该图为了形式上的统一，把神经元的阈值$\\theta_i$看作是以-1为外部输入时的权值，实际上很容易使初学者理解困难。 # 加权求和的数学描述： 用$v_i(t)$，$y_j(t)$，$u_k(t)$分别表示$t$时刻时，神经元$i$的状态（因为神经元的输出决定其状态，所以有时神经元的输出又称为状态，这两个词可能混用），神经元$j$的输出，第$k$个外部输入，则有： $$ \\begin{equation}v_i(t)=\\sum_{j=1}^Na_{ij}y_j(t)+\\sum_{k=1}^Mb_{ik}u_k(t)-\\theta_i\\end{equation} $$ 其中，$\\begin{aligned}\\sum_{j=1}^Na_{ij}y_j(t)\\end{aligned}$表示$y_1$到$y_N$所有神经元的输出与权值乘积的和，代表其他神经元的输出对该神经元的直接影响。 $\\begin{aligned}\\sum_{k=1}^Mb_{ik}u_k(t)\\end{aligned}$表示$u_1$到$u_M$所有外部输出与权值乘积的和，代表外部输入（如人工干涉）对该神经元的直接影响。 $\\theta_i$为神经元$i$的阈值。 显然，所有输入大于阈值时，$v_i(t)>0$，神经元为兴奋状态，否则神经元为抑制状态。 # 神经元的输入输出关系 用$u_i$表示神经元的输入，$y_i$表示神经元的输出，则神经元$i$的输入输出关系表示为： $$ \\begin{equation} y_i=f(u_i) \\end{equation} $$ 其中，$f$为激活函数。该式表明，所有神经元接收输入以后，把它作为参数传递给激活函数，得到该神经元的输出。 # 激活函数 根据函数的图像是否为直线，激活函数可分为线性激活函数1和非线性激活函数。 激活函数的主要作用： 把输入映射到特定区间或特征值，既能决定神经元输出的值域，又可以把输入映射到特征空间（特征值）。 如果激活函数是非线性函数，可以极大地增强神经元的表达能力，建立从输入到输出的非线性映射。（一个非线性函数可以是任意个数的非线性函数与线性函数的叠加，因此具有极强的抽象表达能力） 在上图中，$v_i$只是加权求和的结果，还没有经过激活函数处理，不是最终的输出。 # 常用的 非线性激活函数 对于任意神经元$i$，设$x_i$为输入，则输出为$f(x_i)$。 阶跃函数 $$ \\begin{equation}f(x_i)= \\begin{cases} 0&{x_i<0}\\\\ 1&{x_i\\ge0}\\ \\end{cases} \\end{equation} $$ $$ f(x_i)=\\begin{cases} -1&{x_i<0}\\\\ 1&{x_i\\ge0}\\ \\end{cases} $$ 不管神经元使用什么激活函数，对于神经网络最终的输出，都能够再使用一次阶跃函数实现从原始输出到特征值的映射。 ReLU函数 $$ f(x_i)=\\begin{cases} 0&{x_i<0}\\\\ x_i&{x_i\\ge0}\\ \\end{cases} $$ ReLU函数形式简单，并且没有饱和问题，运算速度快，收敛效果好，常用于卷积神经网络。 S型函数 s型函数具有平滑、渐进和单调性，是最常用的非线性函数。 (1) Sigmoid函数 $$ \\begin{equation}f(x_i)=\\frac{1}{1+e^{-\\alpha x_i}}\\end{equation} $$ 该函数值域为(0,1)，即Sigmoid函数把神经元的输出映射到(0,1)区间。 缺点是，当输入的绝对值大于某个值以后，过快进入饱和状态（函数值趋于0或1，不再有显著的变化），出现梯度消失（梯度趋近于0）的情况，会导致模型训练时收敛缓慢，效果不佳。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://CHmadoka.github.io/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1.png'><link rel="shortcut icon" href=/titleImg.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/file_259987_hu712dd812b62879d39640ef77b16ac818_50548_300x0_resize_q75_box.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>😽</span></figure><div class=site-meta><h1 class=site-name><a href=/>CHmadoka 小屋</a></h1><h2 class=site-description>呆头呆脑的大聪明。在此分享一些比较个人化的内容。</h2></div></header><ol class=menu-social><li><a href=https://github.com/CHmadoka target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>垃圾堆</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/%E9%93%BE%E6%8E%A5/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>链接</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#生物神经元结构>生物神经元结构</a></li><li><a href=#神经元数学模型>神经元数学模型</a><ol><li><a href=#加权求和的数学描述>加权求和的数学描述：</a></li><li><a href=#神经元的输入输出关系>神经元的输入输出关系</a></li><li><a href=#激活函数>激活函数</a><ol><li><a href=#常用的-非线性激活函数>常用的 非线性激活函数</a></li></ol></li></ol></li><li><a href=#神经网络的分类>神经网络的分类</a><ol><li><a href=#按结构分类>按结构分类</a></li><li><a href=#按工作方式分类>按工作方式分类</a></li></ol></li><li><a href=#hebb学习规则>Hebb学习规则</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/><img src=/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1_hu69ae4b20123b44ac4941d164f555be9a_68333_800x0_resize_box_3.png srcset="/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1_hu69ae4b20123b44ac4941d164f555be9a_68333_800x0_resize_box_3.png 800w, /p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1_hu69ae4b20123b44ac4941d164f555be9a_68333_1600x0_resize_box_3.png 1600w" width=800 height=383 loading=lazy alt="Featured image of post ANN 神经网络基础"></a></div><div class=article-details><header class=article-category><a href=/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络
</a><a href=/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/>人工智能
</a><a href=/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/>ANN 神经网络基础</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Apr 09, 2024</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 1 分钟</time></div></footer></div></header><section class=article-content><p>由于一些原因要学习神经网络，发现几乎所有资料的内容都比较含蓄，描述不清晰，结构不完整。几天学习下来很痛苦，决定结合自己的理解写个教程。（结果并非像我一开始想的那样对初学者友好，大概严格的公式理论仍然是绕不开的吧）</p><h2 id=生物神经元结构><a href=#%e7%94%9f%e7%89%a9%e7%a5%9e%e7%bb%8f%e5%85%83%e7%bb%93%e6%9e%84>#</a>
生物神经元结构</h2><p><img src=/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1.png width=581 height=278 srcset="/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1_hu69ae4b20123b44ac4941d164f555be9a_68333_480x0_resize_box_3.png 480w, /p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/1_hu69ae4b20123b44ac4941d164f555be9a_68333_1024x0_resize_box_3.png 1024w" loading=lazy alt=生物神经元结构 class=gallery-image data-flex-grow=208 data-flex-basis=501px></p><h2 id=神经元数学模型><a href=#%e7%a5%9e%e7%bb%8f%e5%85%83%e6%95%b0%e5%ad%a6%e6%a8%a1%e5%9e%8b>#</a>
神经元数学模型</h2><p><img src=/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/2.png width=380 height=252 srcset="/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/2_hu619792b0066f162294981d6fc1d25bbf_31762_480x0_resize_box_3.png 480w, /p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/2_hu619792b0066f162294981d6fc1d25bbf_31762_1024x0_resize_box_3.png 1024w" loading=lazy alt=神经元的数学模型 class=gallery-image data-flex-grow=150 data-flex-basis=361px></p><p>该模型由三部分组成，即<strong>加权求和</strong>、<strong>线性动态系统</strong>和<strong>非线性函数映射</strong>。</p><p>图中，$y_j$表示第$j$个神经元的输出，$\theta_i$表示神经元$i$的阈值，$u_k$（k=1,2,&mldr;,M）为外部输入，$a_{ij}$为神经元$j$到神经元$i$的权值。$b_{ik}$为外部输入$u_k$到神经元$i$的权值，计算时可以把外部输入看做某个神经元的输出（例如BP神经网络中的偏置节点）。</p><p>任意神经元$i$的输入可以看做两部分：从$y_1$到$y_N$代表的其他神经元的输出，以及从$u_1$到$u_M$代表的外部输入。
该图为了形式上的统一，把神经元的阈值$\theta_i$看作是以-1为外部输入时的权值，实际上很容易使初学者理解困难。</p><h3 id=加权求和的数学描述><a href=#%e5%8a%a0%e6%9d%83%e6%b1%82%e5%92%8c%e7%9a%84%e6%95%b0%e5%ad%a6%e6%8f%8f%e8%bf%b0>#</a>
加权求和的数学描述：</h3><p>用$v_i(t)$，$y_j(t)$，$u_k(t)$分别表示$t$时刻时，神经元$i$的状态（因为神经元的输出决定其状态，所以有时<strong>神经元的输出又称为状态</strong>，这两个词可能混用），神经元$j$的输出，第$k$个外部输入，则有：
$$
\begin{equation}v_i(t)=\sum_{j=1}^Na_{ij}y_j(t)+\sum_{k=1}^Mb_{ik}u_k(t)-\theta_i\end{equation}
$$</p><p>其中，$\begin{aligned}\sum_{j=1}^Na_{ij}y_j(t)\end{aligned}$表示$y_1$到$y_N$所有神经元的输出与权值乘积的和，代表其他神经元的输出对该神经元的直接影响。
$\begin{aligned}\sum_{k=1}^Mb_{ik}u_k(t)\end{aligned}$表示$u_1$到$u_M$所有外部输出与权值乘积的和，代表外部输入（如人工干涉）对该神经元的直接影响。
$\theta_i$为神经元$i$的阈值。
显然，所有输入大于阈值时，$v_i(t)>0$，神经元为兴奋状态，否则神经元为抑制状态。</p><h3 id=神经元的输入输出关系><a href=#%e7%a5%9e%e7%bb%8f%e5%85%83%e7%9a%84%e8%be%93%e5%85%a5%e8%be%93%e5%87%ba%e5%85%b3%e7%b3%bb>#</a>
神经元的输入输出关系</h3><p>用$u_i$表示神经元的输入，$y_i$表示神经元的输出，则神经元$i$的输入输出关系表示为：</p><p>$$
\begin{equation}
y_i=f(u_i)
\end{equation}
$$
其中，$f$为激活函数。该式表明，所有神经元接收输入以后，把它作为参数传递给激活函数，得到该神经元的输出。</p><h3 id=激活函数><a href=#%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>#</a>
激活函数</h3><p>根据函数的图像是否为直线，激活函数可分为<strong>线性激活函数<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><strong>和</strong>非线性激活函数</strong>。
激活函数的主要作用：</p><ol><li>把输入映射到特定区间或特征值，既能决定神经元输出的值域，又可以把输入映射到特征空间（特征值）。</li><li>如果激活函数是非线性函数，可以极大地增强神经元的表达能力，建立从输入到输出的非线性映射。（一个非线性函数可以是任意个数的非线性函数与线性函数的叠加，因此具有极强的抽象表达能力）</li></ol><p><img src=/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/2.png width=380 height=252 srcset="/p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/2_hu619792b0066f162294981d6fc1d25bbf_31762_480x0_resize_box_3.png 480w, /p/ann-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/2_hu619792b0066f162294981d6fc1d25bbf_31762_1024x0_resize_box_3.png 1024w" loading=lazy alt=神经元的数学模型 class=gallery-image data-flex-grow=150 data-flex-basis=361px></p><p>在上图中，$v_i$只是加权求和的结果，还没有经过激活函数处理，不是最终的输出。</p><h4 id=常用的-非线性激活函数><a href=#%e5%b8%b8%e7%94%a8%e7%9a%84-%e9%9d%9e%e7%ba%bf%e6%80%a7%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>#</a>
常用的 非线性激活函数</h4><p>对于任意神经元$i$，设$x_i$为输入，则输出为$f(x_i)$。</p><ol><li><strong>阶跃函数</strong></li></ol><p>$$
\begin{equation}f(x_i)=
\begin{cases}
0&{x_i&lt;0}\\
1&{x_i\ge0}\
\end{cases}
\end{equation}
$$
$$
f(x_i)=\begin{cases}
-1&{x_i&lt;0}\\
1&{x_i\ge0}\
\end{cases}
$$
不管神经元使用什么激活函数，对于神经网络最终的输出，都能够再使用一次阶跃函数实现从原始输出到特征值的映射。</p><ol start=2><li><strong>ReLU函数</strong>
$$
f(x_i)=\begin{cases}
0&{x_i&lt;0}\\
x_i&{x_i\ge0}\
\end{cases}
$$</li></ol><p>ReLU函数形式简单，并且没有饱和问题，运算速度快，收敛效果好，常用于卷积神经网络。</p><ol start=3><li><strong>S型函数</strong></li></ol><p>s型函数具有平滑、渐进和单调性，是最常用的非线性函数。</p><p>(1) <strong>Sigmoid函数</strong>
$$
\begin{equation}f(x_i)=\frac{1}{1+e^{-\alpha x_i}}\end{equation}
$$
该函数值域为(0,1)，即Sigmoid函数把神经元的输出映射到(0,1)区间。
缺点是，当输入的绝对值大于某个值以后，过快进入饱和状态（函数值趋于0或1，不再有显著的变化），出现梯度消失（梯度趋近于0）的情况，会导致模型训练时收敛缓慢，效果不佳。</p><p>(2) <strong>双曲线正切函数</strong>
$$
f(x_i)=\frac{1-e^{-\alpha x_i}}{1+e^{-\alpha x_i}}
$$
该函数值域为(-1,1)，即双曲线正切函数把神经元的输出映射到(-1,1)区间。
以上式子中的$\alpha$用于控制函数斜率，实际使用该函数时，通常省略该参数。</p><h2 id=神经网络的分类><a href=#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%88%86%e7%b1%bb>#</a>
神经网络的分类</h2><h3 id=按结构分类><a href=#%e6%8c%89%e7%bb%93%e6%9e%84%e5%88%86%e7%b1%bb>#</a>
按结构分类</h3><ol><li>前馈型</li></ol><p>前馈型神经网络中，可分为不同的层，每一层神经元只与相邻的层相连。神经元的信号传递是单向的，
各神经元接收前一个（层）神经元的输入，并输出给下一个（层）神经元，没有任何反馈。
例如BP神经网络。</p><ol start=2><li>反馈型</li></ol><p>在反馈型神经网络中，存在一些神经元的输出，经过若干个神经元的传递后，又返回作为这些神经元输入的情况。
例如Hopfield神经网络。</p><h3 id=按工作方式分类><a href=#%e6%8c%89%e5%b7%a5%e4%bd%9c%e6%96%b9%e5%bc%8f%e5%88%86%e7%b1%bb>#</a>
按工作方式分类</h3><p>如果神经网络中各神经元同时更新状态（输出），就是<strong>同步</strong>工作方式。
如果神经网络中各神经元按某种顺序，逐个更新状态（输出），每个神经元更新状态时，其他神经元状态不变，就是<strong>异步</strong>工作方式。</p><h2 id=hebb学习规则><a href=#hebb%e5%ad%a6%e4%b9%a0%e8%a7%84%e5%88%99>#</a>
Hebb学习规则</h2><p>如果某一突触两端的神经元同时处于兴奋状态，它们的连接权值应该增强。
两个神经元的输出$y_i$和$y_j$越大，神经元越兴奋，权值就应该增强的越多。
用$w_{ij}$表示神经元$i$与$j$之间的权值，$k$表示时刻，则：
$$
w_{ij}(k+1)=w_{ij}(k)+\alpha y_i(k)y_j(k)\quad (\alpha > 0)
$$
即$\quad更新后的权值=当前权值+权值的改变量$。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>线性函数的通式为 $f=kx+b$&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></section><footer class=article-footer><section class=article-tags><a href=/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/><div class=article-image><img src=/p/bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/1.f51230135fed0c6fbdba1fbf900377ad_hud271e26b886e24e34b864ebb2b9cf980_50512_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post BP神经网络与反向传播算法" data-hash="md5-9RIwE1/tDG+9uh+/kAN3rQ=="></div><div class=article-details><h2 class=article-title>BP神经网络与反向传播算法</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//CHmadoka.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2024 CHmadoka 小屋</section><section class=powerby>Special Thanks to Yukari Chan<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.25.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>